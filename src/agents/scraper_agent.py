import os
import json
import time
import arxiv
import requests
import yaml
from loguru import logger
from typing import List, Dict, Any

class ScraperAgent:
    def __init__(self, config_path: str = "config.yaml"):
        self.config = self._load_config(config_path)
        self.data_dir = self.config['data']['output_dir']
        self.pdf_dir = os.path.join(self.data_dir, self.config['data']['pdf_dir'])
        self.metadata_path = os.path.join(self.data_dir, self.config['data']['metadata_file'])
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(self.pdf_dir, exist_ok=True)
        
        logger.info("ğŸ•µï¸ Scraper Agent initialized.")

    def _load_config(self, path: str) -> Dict[str, Any]:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _get_existing_ids(self) -> List[str]:
        """è®€å–å·²å­˜åœ¨çš„è«–æ–‡ IDï¼Œç”¨æ–¼å¢é‡æ›´æ–°"""
        if not os.path.exists(self.metadata_path):
            return []
        try:
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return [item['id'] for item in data]
        except Exception:
            return []

    def _save_metadata(self, new_data: List[Dict]):
        """å°‡æ–°è³‡æ–™è¿½åŠ åˆ° metadata.json"""
        existing_data = []
        if os.path.exists(self.metadata_path):
            try:
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except json.JSONDecodeError:
                pass
        
        # åˆä½µä¸¦å»é‡ (ä»¥ ID ç‚ºæº–)
        all_data = existing_data + new_data
        # ç°¡å–®å»é‡é‚è¼¯
        seen = set()
        unique_data = []
        for d in all_data:
            if d['id'] not in seen:
                unique_data.append(d)
                seen.add(d['id'])

        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=2)
        logger.success(f"ğŸ’¾ Metadata saved. Total papers: {len(unique_data)}")

    def download_pdf(self, url: str, filename: str) -> bool:
        """ä¸‹è¼‰ PDF ä¸¦åŒ…å«é‡è©¦æ©Ÿåˆ¶"""
        filepath = os.path.join(self.pdf_dir, filename)
        
        # å¢é‡æª¢æŸ¥ï¼šå¦‚æœæª”æ¡ˆå·²å­˜åœ¨ä¸”å¤§å°ä¸ç‚º 0ï¼Œè·³é
        if os.path.exists(filepath) and os.path.getsize(filepath) > 0:
            logger.info(f"â­ï¸  File exists, skipping: {filename}")
            return True

        retries = self.config['scraper']['retry_attempts']
        for attempt in range(retries):
            try:
                response = requests.get(url, stream=True, timeout=10)
                if response.status_code == 200:
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    logger.info(f"â¬‡ï¸  Downloaded: {filename}")
                    time.sleep(self.config['scraper']['sleep_interval'])
                    return True
            except Exception as e:
                logger.warning(f"âš ï¸  Download failed ({attempt+1}/{retries}): {e}")
                time.sleep(2)
        
        logger.error(f"âŒ Failed to download {filename} after retries.")
        return False

    def run(self):
        """åŸ·è¡Œä¸»æµç¨‹"""
        keywords = self.config['scraper']['keywords']
        max_results = self.config['scraper']['max_results']
        
        logger.info(f"ğŸ” Starting search for: {keywords}")
        
        # æ§‹å»ºæŸ¥è©¢èªæ³• (Title OR Abstract)
        query = " OR ".join([f'ti:"{k}" OR abs:"{k}"' for k in keywords])
        
        client = arxiv.Client()
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        new_papers = []
        
        # é–‹å§‹æŠ“å–
        for result in client.results(search):
            paper_id = result.get_short_id()
            filename = f"{paper_id}.pdf"
            
            paper_info = {
                "id": paper_id,
                "title": result.title,
                "authors": [author.name for author in result.authors],
                "summary": result.summary,
                "published": str(result.published),
                "pdf_url": result.pdf_url,
                "local_pdf_path": os.path.join(self.pdf_dir, filename),
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }

            logger.info(f"ğŸ“„ Found: {result.title[:50]}...")
            
            # ä¸‹è¼‰ PDF
            if self.download_pdf(result.pdf_url, filename):
                new_papers.append(paper_info)
        
        # å„²å­˜ Metadata
        if new_papers:
            self._save_metadata(new_papers)
            logger.success(f"âœ… Scraper Agent finished. Processed {len(new_papers)} papers.")
        else:
            logger.info("ğŸ¤· No new papers downloaded.")

if __name__ == "__main__":
    # å–®ç¨æ¸¬è©¦ç”¨
    agent = ScraperAgent()
    agent.run()
