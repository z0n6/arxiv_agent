import os
import json
import yaml
import faiss
import numpy as np
from loguru import logger
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

class VectorAgent:
    def __init__(self, config_path: str = "config.yaml"):
        self.config = self._load_config(config_path)
        self.data_dir = self.config['data']['output_dir']
        
        # è¼¸å…¥ï¼šè§£æå¾Œçš„è«–æ–‡
        self.input_path = os.path.join(self.data_dir, self.config['parser']['output_file'])
        
        # è¼¸å‡ºï¼šFAISS ç´¢å¼• èˆ‡ ID å°ç…§è¡¨
        self.index_path = os.path.join(self.data_dir, self.config['vector']['index_file'])
        self.map_path = os.path.join(self.data_dir, self.config['vector']['chunks_map_file'])
        
        # è¼‰å…¥æ¨¡å‹ (ç¬¬ä¸€æ¬¡åŸ·è¡Œæœƒè‡ªå‹•ä¸‹è¼‰ï¼Œç´„ 80MB)
        model_name = self.config['vector']['model_name']
        logger.info(f"ğŸ§  Loading embedding model: {model_name}...")
        self.model = SentenceTransformer(model_name)
        logger.info("âœ… Model loaded.")

    def _load_config(self, path: str) -> Dict[str, Any]:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _load_parsed_data(self) -> List[Dict]:
        if not os.path.exists(self.input_path):
            logger.error(f"âŒ Parsed file not found: {self.input_path}")
            return []
        with open(self.input_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def create_index(self):
        papers = self._load_parsed_data()
        if not papers:
            return

        all_chunks = []
        metadata_map = {} # ç”¨æ–¼æª¢ç´¢æ™‚åæŸ¥ï¼š ID -> (Paper Title, Text)
        
        # 1. æ•´ç†æ‰€æœ‰è«–æ–‡çš„ Chunks
        global_id = 0
        logger.info("ğŸ“¦ Preparing chunks for embedding...")
        
        for paper in papers:
            paper_title = paper['title']
            for chunk in paper['chunks']:
                all_chunks.append(chunk)
                metadata_map[str(global_id)] = {
                    "paper_id": paper['id'],
                    "title": paper_title,
                    "text": chunk
                }
                global_id += 1

        if not all_chunks:
            logger.warning("No chunks found to embed.")
            return

        # 2. ç”Ÿæˆå‘é‡ (Embedding)
        logger.info(f"ğŸš€ Embedding {len(all_chunks)} chunks... (This may take a while)")
        batch_size = self.config['vector']['batch_size']
        
        # encode æœƒå›å‚³ numpy array
        embeddings = self.model.encode(all_chunks, batch_size=batch_size, show_progress_bar=True)
        
        # 3. å»ºç«‹ FAISS ç´¢å¼•
        # å‘é‡ç¶­åº¦ (all-MiniLM-L6-v2 æ˜¯ 384 ç¶­)
        dimension = embeddings.shape[1] 
        index = faiss.IndexFlatL2(dimension) # ä½¿ç”¨ L2 è·é›¢ (æ­å¼è·é›¢)
        
        # åŠ å…¥å‘é‡
        index.add(embeddings)
        logger.info(f"âœ… Created FAISS index with {index.ntotal} vectors.")

        # 4. å­˜æª”
        faiss.write_index(index, self.index_path)
        with open(self.map_path, 'w', encoding='utf-8') as f:
            json.dump(metadata_map, f, ensure_ascii=False, indent=2)
            
        logger.success(f"ğŸ’¾ Index saved to {self.index_path}")
        logger.success(f"ğŸ’¾ Map saved to {self.map_path}")

    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """(æ¸¬è©¦ç”¨) èªæ„æœå°‹åŠŸèƒ½"""
        if not os.path.exists(self.index_path):
            logger.error("âŒ Index not found. Run create_index() first.")
            return []

        # è¼‰å…¥ç´¢å¼•
        index = faiss.read_index(self.index_path)
        with open(self.map_path, 'r', encoding='utf-8') as f:
            metadata_map = json.load(f)

        # æŸ¥è©¢å‘é‡åŒ–
        query_vector = self.model.encode([query])
        
        # æœå°‹
        distances, indices = index.search(query_vector, top_k)
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx == -1: continue # ç„¡çµæœ
            meta = metadata_map.get(str(idx), {})
            results.append({
                "score": float(distances[0][i]), # è·é›¢è¶Šå°è¶Šç›¸ä¼¼
                "paper_title": meta.get('title'),
                "text": meta.get('text')
            })
            
        return results

if __name__ == "__main__":
    # å–®ç¨åŸ·è¡Œæ™‚ï¼Œå»ºç«‹ç´¢å¼•
    agent = VectorAgent()
    agent.create_index()
