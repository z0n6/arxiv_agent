import os
import json
import fitz  # PyMuPDF
import yaml
import re
from loguru import logger
from typing import List, Dict, Any
from tqdm import tqdm

class ParserAgent:
    def __init__(self, config_path: str = "config.yaml"):
        self.config = self._load_config(config_path)
        self.data_dir = self.config['data']['output_dir']
        self.metadata_path = os.path.join(self.data_dir, self.config['data']['metadata_file'])
        self.output_path = os.path.join(self.data_dir, self.config['parser']['output_file'])
        
        logger.info("ğŸ”¬ Parser Agent initialized.")

    def _load_config(self, path: str) -> Dict[str, Any]:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _load_metadata(self) -> List[Dict]:
        if not os.path.exists(self.metadata_path):
            logger.error(f"âŒ Metadata file not found: {self.metadata_path}")
            return []
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æå–å‡ºçš„æ–‡å­—"""
        # 1. å°‡å¤šå€‹æ›è¡Œç¬¦è™Ÿæ›¿æ›ç‚ºå–®ä¸€ç©ºæ ¼ (å°‡æ®µè½æ¥èµ·ä¾†)
        text = text.replace('\n', ' ')
        # 2. å»é™¤å¤šé¤˜çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        # 3. (å¯é¸) å»é™¤é€£å­—ç¬¦è™Ÿ (ä¾‹å¦‚ "algorithm-" + "ic" -> "algorithmic")
        text = text.replace('- ', '') 
        return text

    def remove_references(self, text: str) -> str:
        """å˜—è©¦å»é™¤ References ä¹‹å¾Œçš„å…§å®¹"""
        # å¸¸è¦‹çš„åƒè€ƒæ–‡ç»æ¨™é¡Œå¯«æ³•
        patterns = [
            r"\nReferences\n", 
            r"\nREFERENCES\n", 
            r"\nBibliography\n"
        ]
        for pattern in patterns:
            split_text = re.split(pattern, text)
            if len(split_text) > 1:
                # å‡è¨­æœ€å¾Œä¸€å€‹éƒ¨åˆ†æ˜¯åƒè€ƒæ–‡ç»ï¼Œå°‡å…¶æ¨æ£„
                # ä½†è¦å°å¿ƒï¼Œæœ‰æ™‚å€™ References æœƒå‡ºç¾åœ¨ä¸­é–“ï¼ˆè¼ƒå°‘è¦‹ï¼‰ï¼Œé€™è£¡æ¡å–ç°¡ç­–ç•¥ï¼š
                # å–æœ€å¾Œä¸€å€‹åˆ†å‰²é»ä¹‹å‰çš„æ‰€æœ‰å…§å®¹
                return pattern.join(split_text[:-1])
        return text

    def chunk_text(self, text: str) -> List[str]:
        """æ»‘å‹•è¦–çª—åˆ†å¡Š (Sliding Window Chunking)"""
        chunk_size = self.config['parser']['chunk_size']
        overlap = self.config['parser']['chunk_overlap']
        
        chunks = []
        start = 0
        text_len = len(text)

        while start < text_len:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            
            # ç§»å‹•è¦–çª— (å‰é€² step = chunk_size - overlap)
            start += (chunk_size - overlap)
        
        return chunks

    def parse_pdf(self, file_path: str) -> str:
        """ä½¿ç”¨ PyMuPDF æå–å…¨æ–‡"""
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ PDF not found: {file_path}")
            return ""
        
        try:
            doc = fitz.open(file_path)
            full_text = ""
            for page in doc:
                full_text += page.get_text()
            return full_text
        except Exception as e:
            logger.error(f"âŒ Failed to parse PDF {file_path}: {e}")
            return ""

    def run(self):
        papers = self._load_metadata()
        if not papers:
            logger.warning("No papers to parse.")
            return

        logger.info(f"ğŸš€ Starting processing for {len(papers)} papers...")
        
        parsed_results = []
        
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
        for paper in tqdm(papers, desc="Parsing PDFs"):
            pdf_path = paper.get('local_pdf_path')
            
            # 1. æå–åŸå§‹æ–‡å­—
            raw_text = self.parse_pdf(pdf_path)
            
            if not raw_text:
                continue

            # 2. æ˜¯å¦å»é™¤åƒè€ƒæ–‡ç»
            if self.config['parser']['ignore_references']:
                raw_text = self.remove_references(raw_text)

            # 3. æ¸…æ´—æ–‡å­—
            cleaned_text = self.clean_text(raw_text)

            # 4. åˆ†å¡Š
            chunks = self.chunk_text(cleaned_text)

            # 5. å»ºç«‹çµæ§‹åŒ–è³‡æ–™
            parsed_paper = {
                "id": paper['id'],
                "title": paper['title'],
                "chunks": chunks,  # é€™è£¡å„²å­˜åˆ‡åˆ†å¥½çš„æ–‡æœ¬åˆ—è¡¨
                "total_chunks": len(chunks),
                "parsed_at": os.path.getmtime(pdf_path) # ç°¡å–®è¨˜éŒ„æ™‚é–“
            }
            parsed_results.append(parsed_paper)

        # å„²å­˜çµæœ
        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(parsed_results, f, ensure_ascii=False, indent=2)
            
        logger.success(f"âœ… Parser Agent finished. Processed {len(parsed_results)} papers.")
        logger.info(f"ğŸ’¾ Results saved to: {self.output_path}")

if __name__ == "__main__":
    agent = ParserAgent()
    agent.run()
